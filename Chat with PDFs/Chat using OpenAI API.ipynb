{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ba6108-588a-470b-8ea3-86521985ec4c",
   "metadata": {},
   "source": [
    "# Chat with PDFs using ChatGPT & OpenAI GPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1d5ed-de6d-4edb-8c02-5abe8631c5bd",
   "metadata": {},
   "source": [
    "This is a supplementary python notebook for the blog - https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/. We dive into a detailed code tutorial on how to chat with all kinds of PDF files using OpenAI GPT API and use it for PDF automations / chatbots.\n",
    "\n",
    "* We will chat with PDFs using just a few lines of Python code.\n",
    "* We will chat with large PDF files using ChatGPT API and LangChain.\n",
    "* We will build an automation to sort PDF files based on their contents.\n",
    "* We will go through examples of building more automations for tasks involving PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b5d6-dc01-4ea3-a4aa-0b9065894da3",
   "metadata": {},
   "source": [
    "## Chat with PDF using ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49317bea-fcb7-4473-bf1c-1603ca18d67b",
   "metadata": {},
   "source": [
    "Let us now chat with our first PDF using OpenAI's GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ef09a-3a36-4e14-8477-6204a11e3495",
   "metadata": {},
   "source": [
    "We are going to converse with a resume PDF to demonstrate this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057ed90-7e39-41e0-b2ef-fa92dba71b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 1 - Read the PDF File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820fd2c-62e2-4f9a-a45b-151222a7cc2b",
   "metadata": {},
   "source": [
    "We follow different approaches based on whether the PDF is scanned or digital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cea90-d384-4a2b-a7fa-8f00ccc21b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Approach 1 : Read Digital PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install PyPDF2\n",
    "!pipenv install pdf2image \n",
    "!pipenv install Pillow \n",
    "!pipenv install pytesseract\n",
    "!pipenv install openai\n",
    "!pipenv install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd36aa92-9c07-4c79-b107-0ac6b90b6e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTIONAL  (EXPERIENCED)   \n",
      "IM A . SAMPLE I  \n",
      "1234 North 55 Street  \n",
      "Bellevue, Nebraska 68005  \n",
      "(402) 292 -2345  \n",
      "imasample1@xxx.com  \n",
      " \n",
      "SUMMARY OF QUALIFICATIONS  \n",
      "Exceptionally well organized and resourceful Professional  with more than six years experience and a \n",
      "solid academic background in accounting and financial management; excellent analytical and problem \n",
      "solving skills; able to handle multiple projects while pr oducing high quality work in a fast -paced, \n",
      "deadline -oriented environment.  \n",
      " \n",
      "EDUCATION  \n",
      "Bachelor of Science , Bellevue University, Bellevue, NE (In Progress)  \n",
      " Major:  Accounting  Minor:  Computer Information Systems  \n",
      " Expected Graduation Date:  January, 20xx  GPA  to date:  3.95/4.00  \n",
      " \n",
      "PROFESSIONAL ACCOMPLISHMENTS  \n",
      "Accounting and Financial Management  \n",
      " Developed and maintained accounting records for up to fifty bank accounts.  \n",
      " Formulated monthly and year -end financial statements and generated various payroll records, \n",
      "including federal and state payroll reports, annual tax reports, W -2 and 1099 forms, etc.  \n",
      " Tested accuracy of account balances and prepared supporting documentation for submission during a \n",
      "comprehensive three -year audit of financial operations.  \n",
      " Formulated int ricate pro -forma budgets.  \n",
      " Calculated and implemented depreciation/amortization schedules.  \n",
      "Information Systems Analysis and Problem Solving  \n",
      " Converted manual to computerized accounting systems for two organizations.  \n",
      " Analyzed and successfully reprogrammed sof tware to meet customer requirements.  \n",
      " Researched and corrected problems to assure effective operation of newly computerized systems.  \n",
      " \n",
      "WORK HISTORY  \n",
      "Student Intern , Financial Accounting Development Program, Mutual of Omaha, Omaha, NE  \n",
      "(Summer 20xx)  \n",
      "Accounting Coordinator , Nebraska Special Olympics, Omaha, NE (20xx -20xx)  \n",
      "Bookkeeper , SMC, Inc., Omaha, NE (20xx – 20xx)  \n",
      "Bookkeeper , First United Methodist Church, Altus, OK (20xx – 20xx)  \n",
      " \n",
      "PROFESSIONAL AFFILIATION  \n",
      "Member,  IMA, Bellevue University Student Chapter  \n",
      " \n",
      "COMP UTER SKILLS  \n",
      " Proficient in MS Office (Word, Excel, PowerPoint, Outlook), QuickBooks  \n",
      " Basic Knowledge of MS Access, SQL, Visual Basic, C++  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file_obj = open('resume-sample.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "num_pages = len(pdf_reader.pages)\n",
    "detected_text = ''\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_obj = pdf_reader.pages[page_num]\n",
    "    detected_text += page_obj.extract_text() + '\\n\\n'\n",
    "\n",
    "pdf_file_obj.close()\n",
    "\n",
    "print(detected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d902485-c9b3-4872-a093-6d06aad18cf2",
   "metadata": {},
   "source": [
    "##### Approach 2 : Read Scanned PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7277de2-ca00-4ab5-8c9c-733d7ef0a73d",
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFInfoNotInstalledError",
     "evalue": "Unable to get page count. Is poppler installed and in PATH?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/pdf2image/pdf2image.py:568\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout)\u001b[0m\n\u001b[1;32m    567\u001b[0m     env[\u001b[39m\"\u001b[39m\u001b[39mLD_LIBRARY_PATH\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m poppler_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m env\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mLD_LIBRARY_PATH\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 568\u001b[0m proc \u001b[39m=\u001b[39m Popen(command, env\u001b[39m=\u001b[39;49menv, stdout\u001b[39m=\u001b[39;49mPIPE, stderr\u001b[39m=\u001b[39;49mPIPE)\n\u001b[1;32m    570\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pdfinfo'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/Chat using OpenAI API.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytesseract\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m image \u001b[39m=\u001b[39m pdf2image\u001b[39m.\u001b[39;49mconvert_from_path(\u001b[39m'\u001b[39;49m\u001b[39mresume-sample.pdf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m pagenumber, page \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(image):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     detected_text \u001b[39m=\u001b[39m pytesseract\u001b[39m.\u001b[39mimage_to_string(page)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/pdf2image/pdf2image.py:127\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(poppler_path, PurePath):\n\u001b[1;32m    125\u001b[0m     poppler_path \u001b[39m=\u001b[39m poppler_path\u001b[39m.\u001b[39mas_posix()\n\u001b[0;32m--> 127\u001b[0m page_count \u001b[39m=\u001b[39m pdfinfo_from_path(\n\u001b[1;32m    128\u001b[0m     pdf_path, userpw, ownerpw, poppler_path\u001b[39m=\u001b[39;49mpoppler_path\n\u001b[1;32m    129\u001b[0m )[\u001b[39m\"\u001b[39m\u001b[39mPages\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    131\u001b[0m \u001b[39m# We start by getting the output format, the buffer processing function and if we need pdftocairo\u001b[39;00m\n\u001b[1;32m    132\u001b[0m parsed_fmt, final_extension, parse_buffer_func, use_pdfcairo_format \u001b[39m=\u001b[39m _parse_format(\n\u001b[1;32m    133\u001b[0m     fmt, grayscale\n\u001b[1;32m    134\u001b[0m )\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/pdf2image/pdf2image.py:594\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[39mreturn\u001b[39;00m d\n\u001b[1;32m    593\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mraise\u001b[39;00m PDFInfoNotInstalledError(\n\u001b[1;32m    595\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to get page count. Is poppler installed and in PATH?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    596\u001b[0m     )\n\u001b[1;32m    597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     \u001b[39mraise\u001b[39;00m PDFPageCountError(\n\u001b[1;32m    599\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to get page count.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merr\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m     )\n",
      "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m: Unable to get page count. Is poppler installed and in PATH?"
     ]
    }
   ],
   "source": [
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "image = pdf2image.convert_from_path('resume-sample.pdf')\n",
    "for pagenumber, page in enumerate(image):\n",
    "    detected_text = pytesseract.image_to_string(page)\n",
    "    print(detected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058399a7-019a-4cbd-a276-a45b6d7a861d",
   "metadata": {},
   "source": [
    "#### Step 2 - First Chat with PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d602b7b-1335-49fe-a13a-2243e4af3a89",
   "metadata": {},
   "source": [
    "Let us ask the LLM to suggest jobs that this person will be suitable for based on his resume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a3256-3bbf-4653-927e-88e33b8fb56b",
   "metadata": {},
   "source": [
    "Firstly, We import the os and openai library and define our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e74c9f1-0fbb-4ffa-a296-44da367e6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass \n",
    "\n",
    "openai.api_key = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04936e0-574b-42c6-a0b9-d8a8f83368d9",
   "metadata": {},
   "source": [
    "Choosing the ideal model while using OpenAI's python library depends on your use case and specific requirements. We recommend going through the list of available models and learning the pros and cons of each of the available models. You can access the list of available models as follows - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb9dc66-c557-4516-aa71-75bb3697c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>permission</th>\n",
       "      <th>root</th>\n",
       "      <th>parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-similarity-curie-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1651172507</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>[{'id': 'modelperm-GcRzwghQAA4RMbu5mzeVLaaS', ...</td>\n",
       "      <td>text-similarity-curie-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4</td>\n",
       "      <td>model</td>\n",
       "      <td>1687882411</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-kMXwdWZD1zTr5s2zZhMZMCw0', ...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-0314</td>\n",
       "      <td>model</td>\n",
       "      <td>1687882410</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-QANFnZSlXuyuzFGziCgkrVRf', ...</td>\n",
       "      <td>gpt-4-0314</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>babbage</td>\n",
       "      <td>model</td>\n",
       "      <td>1649358449</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-h574xGeqWyBeFDDKaoVTC4CO', ...</td>\n",
       "      <td>babbage</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text-babbage-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1649364043</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-YABzYWjC1kS6M2BnI6Fr9vuS', ...</td>\n",
       "      <td>text-babbage-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>curie-similarity</td>\n",
       "      <td>model</td>\n",
       "      <td>1651172510</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>[{'id': 'modelperm-7JoCxEzsKwSxMVoWuxLpmJxR', ...</td>\n",
       "      <td>curie-similarity</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>davinci</td>\n",
       "      <td>model</td>\n",
       "      <td>1649359874</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-RcfCH2MkO5NP9C7wx6kdYnIT', ...</td>\n",
       "      <td>davinci</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>code-davinci-edit-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1649880484</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-T8Ie7SvlPyvtsDvPlfC8DftZ', ...</td>\n",
       "      <td>code-davinci-edit-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>curie-instruct-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1649364042</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-rTxpdy2DwwUp38frYQFsj5OC', ...</td>\n",
       "      <td>curie-instruct-beta</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-3.5-turbo-instruct-0914</td>\n",
       "      <td>model</td>\n",
       "      <td>1694122472</td>\n",
       "      <td>system</td>\n",
       "      <td>[{'id': 'modelperm-MrSDYnG6Tr5wLfenxPRaqccH', ...</td>\n",
       "      <td>gpt-3.5-turbo-instruct-0914</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id object     created    owned_by  \\\n",
       "0    text-similarity-curie-001  model  1651172507  openai-dev   \n",
       "1                        gpt-4  model  1687882411      openai   \n",
       "2                   gpt-4-0314  model  1687882410      openai   \n",
       "3                      babbage  model  1649358449      openai   \n",
       "4             text-babbage-001  model  1649364043      openai   \n",
       "5             curie-similarity  model  1651172510  openai-dev   \n",
       "6                      davinci  model  1649359874      openai   \n",
       "7        code-davinci-edit-001  model  1649880484      openai   \n",
       "8          curie-instruct-beta  model  1649364042      openai   \n",
       "9  gpt-3.5-turbo-instruct-0914  model  1694122472      system   \n",
       "\n",
       "                                          permission  \\\n",
       "0  [{'id': 'modelperm-GcRzwghQAA4RMbu5mzeVLaaS', ...   \n",
       "1  [{'id': 'modelperm-kMXwdWZD1zTr5s2zZhMZMCw0', ...   \n",
       "2  [{'id': 'modelperm-QANFnZSlXuyuzFGziCgkrVRf', ...   \n",
       "3  [{'id': 'modelperm-h574xGeqWyBeFDDKaoVTC4CO', ...   \n",
       "4  [{'id': 'modelperm-YABzYWjC1kS6M2BnI6Fr9vuS', ...   \n",
       "5  [{'id': 'modelperm-7JoCxEzsKwSxMVoWuxLpmJxR', ...   \n",
       "6  [{'id': 'modelperm-RcfCH2MkO5NP9C7wx6kdYnIT', ...   \n",
       "7  [{'id': 'modelperm-T8Ie7SvlPyvtsDvPlfC8DftZ', ...   \n",
       "8  [{'id': 'modelperm-rTxpdy2DwwUp38frYQFsj5OC', ...   \n",
       "9  [{'id': 'modelperm-MrSDYnG6Tr5wLfenxPRaqccH', ...   \n",
       "\n",
       "                          root parent  \n",
       "0    text-similarity-curie-001   None  \n",
       "1                        gpt-4   None  \n",
       "2                   gpt-4-0314   None  \n",
       "3                      babbage   None  \n",
       "4             text-babbage-001   None  \n",
       "5             curie-similarity   None  \n",
       "6                      davinci   None  \n",
       "7        code-davinci-edit-001   None  \n",
       "8          curie-instruct-beta   None  \n",
       "9  gpt-3.5-turbo-instruct-0914   None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "models = openai.Model.list()\n",
    "modelsdf = pd.DataFrame(models[\"data\"])\n",
    "modelsdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ab193-5ecd-48d4-a4b6-d4353262a80c",
   "metadata": {},
   "source": [
    "Next, we append our query - \"give a list of jobs suitable for the above resume\" to the extracted PDF text and send this as the user_msg. The detected_text variable already contains the data extracted from the PDF. We will simply append our query here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d225142-7b7d-4ca2-9acb-b7fd32eb22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'give a list of jobs suitable for the above resume.'\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3b7e7-20a3-4930-bb66-fa6c9fdd6dc1",
   "metadata": {},
   "source": [
    "We also add a relevant system_msg to refine the behavior of the AI assistant. In our case, a useful system message can be \"You are a helpful career advisor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d5ca7a-9d31-4eb4-88ef-ac836fa3a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = 'You are a helpful career advisor.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc733f-b44c-46f3-a975-c6462b116c3a",
   "metadata": {},
   "source": [
    "We send the request to get our first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63bb745-1e83-4e0b-a59e-03b9e08e004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": user_msg}]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b908b03-b3f8-4493-8c7b-5ed9b4b12048",
   "metadata": {},
   "source": [
    "Once the request is complete, the response object will contain the response from the LLM. We can view it by accessing the 'choices' attribute in the response object as follows -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65372e2-17e4-4f25-ad14-c7225f89e0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the resume, here is a list of potential job opportunities suitable for the candidate:\n",
      "\n",
      "1. Accountant\n",
      "2. Financial Analyst\n",
      "3. Auditing Associate\n",
      "4. Financial Reporting Specialist\n",
      "5. Bookkeeper\n",
      "6. Accounting Coordinator\n",
      "7. Budget Analyst\n",
      "8. Financial Systems Analyst\n",
      "9. Payroll Specialist\n",
      "10. Tax Consultant\n",
      "11. Senior Accountant\n",
      "12. Internal Auditor\n",
      "13. Financial Planner\n",
      "14. Data Analyst\n",
      "15. Cost Accountant\n",
      "\n",
      "Remember to tailor the resume for each job application to highlight relevant skills and experiences.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c13aec-3949-4cc3-a1cc-423247d2b1df",
   "metadata": {},
   "source": [
    "#### Step 3 : Continuing the Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9a9a3-9fea-4d39-b71c-5697ef868863",
   "metadata": {},
   "source": [
    "Often, we would want to have conversations with the LLM which are more than just a pair of a single prompt and a single response. Let us now learn how to use our past conversation history to continue the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4239ad3-27b1-4a47-a092-005132d5e991",
   "metadata": {},
   "source": [
    "To simplify the implementation, we define the following function for calling the OpenAI GPT API from now on -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee6681b7-d81d-4f23-9508-0852ae81ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_chat(system_message, user_assistant_messages):\n",
    "  \n",
    "  system_msg = [{\"role\": \"system\", \"content\": system_message}]\n",
    "  \n",
    "  user_assistant_msgs = [{\"role\": \"assistant\", \"content\": user_assistant_messages[i]} if i % 2 else {\"role\": \"user\", \"content\": user_assistant_messages[i]} for i in range(len(user_assistant_messages))]\n",
    "\n",
    "  allmsgs = system_msg + user_assistant_msgs\n",
    "  response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                          messages=allmsgs)\n",
    "  \n",
    "  return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb502de-47c6-48d4-9768-f85d202a8619",
   "metadata": {},
   "source": [
    "The function accepts -\n",
    "\n",
    "* system_message (string) : This acts as the system_msg\n",
    "* user_assistant_messages (list) : This list contains user prompts and model responses in alternating order. This is also the order in which they occur in the conversation.\n",
    "\n",
    "The function internally makes the API call to generate and return a new response based on the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c10e06-53f9-4f3a-a903-abc1252e07f8",
   "metadata": {},
   "source": [
    "Let us now use this function to continue our previous conversation, and find out the highest paying jobs out of the ones recommended in the first response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f299e-6cc6-4a75-bb9c-44bd309fb2e8",
   "metadata": {},
   "source": [
    "We will use the same system message (system_msg) used in previous call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef1646-2c25-47cd-9952-001d0034f98d",
   "metadata": {},
   "source": [
    "We create user_assistant_messages list as follows - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da14a5e-ef43-4e5e-9ff5-9aeee2657620",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg1 = user_msg\n",
    "model_response1 = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "user_msg2 = 'based on the suggestions, choose the 3 jobs with highest average salary'\n",
    "user_assistant_msgs = [user_msg1, model_response1, user_msg2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5818d1-85f5-4d59-a6e6-84fa5f92a964",
   "metadata": {},
   "source": [
    "Note that we used the original prompt as the first user message (user_msg1), the response to that prompt as the first model response message (model_response1), and our new prompt as the second user message (user_msg2).\n",
    "\n",
    "Finally, we add them to the user_assistant_messages list in order of their occurrence in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ff2a6-b81c-4213-a9a2-2bb68d6ec16f",
   "metadata": {},
   "source": [
    "We now call the continue_chat() function to get the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4946e85-abe9-43e3-8ce6-75be57a91cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = continue_chat(system_msg, user_assistant_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18388927-b53e-4b69-8f53-b8d120da1851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the suggestions provided and considering the average salary, the three jobs with the highest average salary are:\n",
      "\n",
      "1. Financial Analyst\n",
      "2. Senior Accountant\n",
      "3. Financial Planner\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0aa66a-6707-4c33-9995-58b63d06340c",
   "metadata": {},
   "source": [
    "## Chat with Large PDFs using ChatGPT API and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9b7ce-05a2-458c-b9b6-5b98b07d4173",
   "metadata": {},
   "source": [
    "The code tutorial shown above fails for very large PDFs. Let us illustrate this with an example. We will try to chat with BCG's \"2022 Annual Sustainability Report\", a large PDF published by the Boston Consulting Group (BCG) on their general impact in the industry. We execute the code shown below -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc1dfb47-ef56-4a75-a2a4-1423a00d1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251848\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file_obj = open('bcg-2022-annual-sustainability-report-apr-2023.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "num_pages = len(pdf_reader.pages)\n",
    "detected_text = ''\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_obj = pdf_reader.pages[page_num]\n",
    "    detected_text += page_obj.extract_text() + '\\n\\n'\n",
    "\n",
    "pdf_file_obj.close()\n",
    "print(len(detected_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d084d-04f5-4880-85f9-cb3ae664a7c0",
   "metadata": {},
   "source": [
    "We can see that the PDF is super large, and the length of the detected_text string variable is roughly 250k.\n",
    "\n",
    "Let us now try chatting with the PDF -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "972c4d72-ab03-4b06-afdb-5a3b37aa5985",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 54957 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/Chat using OpenAI API.ipynb Cell 44\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X61sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X61sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39msummarize this PDF in 500 words.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X61sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m'''\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X61sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m user_msg \u001b[39m=\u001b[39m detected_text \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m query\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X61sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X61sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                         messages\u001b[39m=\u001b[39;49m[{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: system_msg},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                          {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: user_msg}])\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 54957 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "system_msg = ''\n",
    "\n",
    "query = '''\n",
    "summarize this PDF in 500 words.\n",
    "'''\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                         {\"role\": \"user\", \"content\": user_msg}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b64e1-ad09-4aa3-a633-9e2a9ae9bd18",
   "metadata": {},
   "source": [
    "We get an error message saying that we have hit the prompt length threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364eebdc-44b9-4d49-a0fb-ec0216620db6",
   "metadata": {},
   "source": [
    "This happens because for large PDFs with lots of text, the request payload we send to OpenAI becomes too large, and OpenAI returns an error saying that we have hit the prompt length threshold.\n",
    "\n",
    "Let us now learn how to remove this bottleneck.\n",
    "\n",
    "Enter LangChain. LangChain is an innovative technology that functions as a bridge -  linking large language models (LLMs) with practical applications like Python programming, PDFs, CSV files, or databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133610e2-7f07-4c12-9f97-f4ddabd6954e",
   "metadata": {},
   "source": [
    "Let us import the required dependencies and get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install langchain\n",
    "!pipenv install pypdf\n",
    "!pipenv install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c49eb2b-3708-4926-b0bd-4b38d901a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464f6f9-1c02-448f-a8e9-374e896cb8e7",
   "metadata": {},
   "source": [
    "We load the PDF using PyPDF loader for LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f133dabb-cffa-4488-bfa2-2eaefe6eb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"bcg-2022-annual-sustainability-report-apr-2023.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c1956-221f-47ab-9628-cf1911da53fd",
   "metadata": {},
   "source": [
    "We will perform chunking and split the text using LangChain text splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd94780d-8814-4991-a245-cee2019e2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f5ad8-22a1-4892-8061-6a49879ae312",
   "metadata": {},
   "source": [
    "We create a vector database using the chunks. We will save it the database for future use as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "591f777e-a3c8-4f92-ba74-87664c62748e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-4HrUm***************************************H9mW. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/Chat using OpenAI API.ipynb Cell 55\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m directory \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mindex_store\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vector_index \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39;49mfrom_documents(texts, OpenAIEmbeddings())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vector_index\u001b[39m.\u001b[39msave_local(directory)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/vectorstores/base.py:417\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    416\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39;49mmetadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/vectorstores/faiss.py:602\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[1;32m    577\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    583\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[1;32m    584\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \n\u001b[1;32m    586\u001b[0m \u001b[39m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[1;32m    603\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__from(\n\u001b[1;32m    604\u001b[0m         texts,\n\u001b[1;32m    605\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    610\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \n\u001b[1;32m    473\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:367\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    364\u001b[0m     _iter \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(tokens), _chunk_size)\n\u001b[1;32m    366\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m _iter:\n\u001b[0;32m--> 367\u001b[0m     response \u001b[39m=\u001b[39m embed_with_retry(\n\u001b[1;32m    368\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    369\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens[i : i \u001b[39m+\u001b[39;49m _chunk_size],\n\u001b[1;32m    370\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invocation_params,\n\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    372\u001b[0m     batched_embeddings\u001b[39m.\u001b[39mextend(r[\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    374\u001b[0m results: List[List[List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(texts))]\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:107\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     response \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m _check_response(response, skip_empty\u001b[39m=\u001b[39membeddings\u001b[39m.\u001b[39mskip_empty)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m _embed_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:104\u001b[0m, in \u001b[0;36membed_with_retry.<locals>._embed_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_embed_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 104\u001b[0m     response \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m _check_response(response, skip_empty\u001b[39m=\u001b[39membeddings\u001b[39m.\u001b[39mskip_empty)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_resources/embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-4HrUm***************************************H9mW. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798c771-1a80-4dc7-8554-dec20b73c628",
   "metadata": {
    "tags": []
   },
   "source": [
    "We now load the database. Using the database, we configure a retriever and then create a chat object. This chat object (qa_interface) will be used to chat with the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8625c8b-8cb4-4893-8c2b-0a7686ad359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba03911-8774-428a-818d-471290ae5379",
   "metadata": {},
   "source": [
    "We can now start chatting with the PDF. Let us ask the PDF to list measures taken to address diseases occurring in developing industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec8058-5ca5-463b-bca0-d82655fe927e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = qa_interface(\"List measures taken to address diseases occuring in developing industries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba31977-c58a-4ce9-9e02-ca10441abfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98144ad4-c83b-457d-a218-d83fcf2d87fd",
   "metadata": {},
   "source": [
    "So far, we've used the RetrievalQA chain, a LangChain type for pulling document pieces from a vector store and asking one question about them. But, sometimes we need to have a full conversation about a document, including referring to topics we've already talked about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d89fa5-8647-4c60-9900-ce80e63e5edf",
   "metadata": {},
   "source": [
    "Thankfully, LangChain has us covered. To make this possible, our system needs a memory or conversation history.  Instead of the RetrievalQA chain, we'll use the ConversationalRetrievalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f16b7-d173-4e71-9754-c7afab10afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_interface = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0), retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949d78f-9e99-457f-a78b-8b4ad86d926f",
   "metadata": {},
   "source": [
    "Let's ask the PDF to reveal the context in which Morocco is mentioned in the report.\n",
    "\n",
    "'chat_history' parameter is a list contains past conversation history. For the first message, this list will be empty.\n",
    "\n",
    "'question' parameter is used to send our message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea0fa8-ef31-4e8d-be1f-7606d17db122",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"in what context is Morocco mentioned in the report?\"\n",
    "result = conv_interface({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e09a97-ceee-4320-a5f3-15dbce7bb1ed",
   "metadata": {},
   "source": [
    "Let us now continue the conversation by updating the chat_history variable and ask the PDF to give some statistics around this. We append the messages in order of appearance in the conversation. We first append our initial message followed by the first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef65d2-2a17-4b22-811e-d1e3cc386654",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854acef-2c8c-4518-91d8-97b16d11d0fa",
   "metadata": {},
   "source": [
    "We now add our new question along with the updated chat_history to continue the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f84982-b5f7-4f38-8ff2-9e73663faa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give some statistics around this.\"\n",
    "result = conv_interface({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9955e9-2404-456c-a77d-ebd7073eefe4",
   "metadata": {},
   "source": [
    "The result uses the context gained by knowing the conversation history, and provides another great response! We can keep updating the chat_history variable and further continue our conversation using this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ddf19-3634-4554-b9a0-22fcaa15eef8",
   "metadata": {},
   "source": [
    "## Build PDF Automations using OpenAI GPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422d523-b6ca-4cab-b815-9003eb8fe392",
   "metadata": {},
   "source": [
    "Let us now explore automations involving PDF tasks that can be implemented using GPT API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688353d-83fb-4009-b07c-69d51224d17f",
   "metadata": {},
   "source": [
    "#### Automation 1 - Document Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ee803-ee65-4847-bd0a-b1238b4089ae",
   "metadata": {},
   "source": [
    "GPT-3.5 is excellent at extracting data from documents. Let us try to extract data from an invoice using it. We are going to extract the following fields in JSON format - invoice_date, invoice_number, seller_name, seller_address, total_amount, and each line item present in the invoice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7d3f4-57fc-44b7-b5cc-91ad48ff12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "image = pdf2image.convert_from_path('invoice.pdf')\n",
    "for pagenumber, page in enumerate(image):\n",
    "    detected_text = pytesseract.image_to_string(page)\n",
    "    \n",
    "system_msg = 'You are an invoice processing solution.'\n",
    "\n",
    "query = '''\n",
    "extract data from above invoice and return only the json containing the following -\n",
    "invoice_date, invoice_number, seller_name, seller_address, total_amount, and each line item present in the invoice.\n",
    "json=\n",
    "'''\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                         {\"role\": \"user\", \"content\": user_msg}])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c475d48-81cd-4608-9800-0e61dffec99f",
   "metadata": {},
   "source": [
    "The json here is essentially a json dump - it is a text string which is in the correct json format, but is not a json variable yet.\n",
    "\n",
    "Let us convert this response to a json variable, which happens by adding just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aef0fe-705d-48a1-a375-d257f2b5b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "invoice_json = json.loads(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03085da9-53b1-43bb-93ae-f1a9c5d394be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_json = json.dumps(invoice_json, indent=2)\n",
    "print(pretty_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1c40f-b279-4019-96e9-fc82708a7b6a",
   "metadata": {},
   "source": [
    "#### Automation 2 - Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f6647-52c5-4dde-8fb8-67f458bdd861",
   "metadata": {},
   "source": [
    "Let us consider an example. Say we have a lot of files which are either invoices or receipts. We want to classify and sort these documents based on their type.\n",
    "\n",
    "Doing this is easy using GPT API.\n",
    "\n",
    "We create simple python functions to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102e366-6a1c-4266-9da0-6d9ec447f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "\n",
    "def list_files_only(directory_path):\n",
    "    if os.path.isdir(directory_path):\n",
    "        file_list = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "        file_list = [file for file in file_list if \".pdf\" in file]\n",
    "        return file_list\n",
    "    else:\n",
    "        return f\"{directory_path} is not a directory\"\n",
    "\n",
    "def classify(file_name):\n",
    "    \n",
    "    image = pdf2image.convert_from_path(file_name)\n",
    "    for pagenumber, page in enumerate(image):\n",
    "        detected_text = pytesseract.image_to_string(page)\n",
    "    \n",
    "    system_msg = 'You are an accounts payable expert.'\n",
    "\n",
    "    query = '''\n",
    "    Classify this document and return one of these two document types as response - [Invoices, Receipts]\n",
    "    Return only the document type in the response.\n",
    "\n",
    "    Document Type = \n",
    "    '''\n",
    "\n",
    "    user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                             {\"role\": \"user\", \"content\": user_msg}])\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def move_file(current_path, new_folder):\n",
    "    if os.path.isfile(current_path) and os.path.isdir(new_folder):\n",
    "        file_name = os.path.basename(current_path)\n",
    "        new_path = os.path.join(new_folder, file_name)\n",
    "        shutil.move(current_path, new_path)\n",
    "        print(f'File moved to {new_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8e4a0-2b9e-475a-8e1a-51b97b5b77e7",
   "metadata": {},
   "source": [
    "We create two folders labelled 'Invoices' & 'Receipts', in the folder where the unclassified invoices & receipts are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27097f34-7bbe-46d1-9dd7-65db903725e2",
   "metadata": {},
   "source": [
    "Let us execute the code now to classify these files and sort them into separate folders based on the document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03846c-25b1-46e8-a166-2e6e58412f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = list_files_only('invoices and receipts/')\n",
    "for doc in list_of_files:\n",
    "    current_path = 'invoices and receipts/' + doc\n",
    "    doc_type = classify(current_path)\n",
    "    new_path = 'invoices and receipts/' + doc_type\n",
    "    move_file(current_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a55430-b88c-4d3a-bbc9-6e67f0f6051b",
   "metadata": {},
   "source": [
    "Upon execution, the code sorts these files perfectly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b610220-b182-4edb-8cf4-ed2c640daeb6",
   "metadata": {},
   "source": [
    "#### Automation 3 - Recipe Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ecc623-9789-4a51-9454-d3a1beb98395",
   "metadata": {},
   "source": [
    "We can even feed our favorite cookbooks to GPT API, and ask it to give recipe recommendations based on our inputs. Let us look at an example. We use the Brakes' Meals n More recipe cookbook, and talk to it using LangChain. Let us ask it to give recommendations based on the ingredients we have at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d44353-9b08-4d09-82c0-dde36470bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "directory = 'index_store'\n",
    "\n",
    "loader = PyPDFLoader(\"meals-more-recipes.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])\n",
    "\n",
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)\n",
    "\n",
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "response = qa_interface(\"\"\"\n",
    "I have a lot of broccoli and tomatoes at home. \n",
    "Recommend recipe for some meal I can make at home using these.\n",
    "\"\"\")\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e188d-e9f1-40b5-950f-f9aeb4d7121f",
   "metadata": {},
   "source": [
    "Upon execution, the PDF recommends a recipe for a meal that can be prepared using the mentioned ingredients!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f154b91-9bde-463b-a584-ebe146902efc",
   "metadata": {},
   "source": [
    "#### Automation 4 - Automated Test Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85030b2-0966-486d-854a-95d2bdc9d771",
   "metadata": {},
   "source": [
    "You can feed textbooks and automate creation of complete question papers and tests using GPT API. The LLM can even generate the marking scheme for you!\n",
    "We use the textbook Advanced High-School Mathematics by David B. Surowski and ask the LLM to create a question paper with a marking scheme for a particular chapter in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35da741-fc85-40b1-ae29-8d70a6e47738",
   "metadata": {},
   "source": [
    "We execute the below code - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d9202-2058-4fb5-a859-5bf213673789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "directory = 'index_store'\n",
    "\n",
    "loader = PyPDFLoader(\"further.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])\n",
    "\n",
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)\n",
    "\n",
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "response = qa_interface(\"\"\"\n",
    "list 5 questions of 20 marks total of varying difficuly and weightage based on the topic \"Euclidian Geometry\"\n",
    "\"\"\")\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f36101-9ae6-4854-8912-341873902998",
   "metadata": {},
   "source": [
    "The LLM reads the PDF textbook and create the question paper for us!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
