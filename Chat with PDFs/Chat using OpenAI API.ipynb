{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ba6108-588a-470b-8ea3-86521985ec4c",
   "metadata": {},
   "source": [
    "# Chat with PDFs using ChatGPT & OpenAI GPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1d5ed-de6d-4edb-8c02-5abe8631c5bd",
   "metadata": {},
   "source": [
    "This is a supplementary python notebook for the blog - https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/. We dive into a detailed code tutorial on how to chat with all kinds of PDF files using OpenAI GPT API and use it for PDF automations / chatbots.\n",
    "\n",
    "* We will chat with PDFs using just a few lines of Python code.\n",
    "* We will chat with large PDF files using ChatGPT API and LangChain.\n",
    "* We will build an automation to sort PDF files based on their contents.\n",
    "* We will go through examples of building more automations for tasks involving PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b5d6-dc01-4ea3-a4aa-0b9065894da3",
   "metadata": {},
   "source": [
    "## Chat with PDF using ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49317bea-fcb7-4473-bf1c-1603ca18d67b",
   "metadata": {},
   "source": [
    "Let us now chat with our first PDF using OpenAI's GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ef09a-3a36-4e14-8477-6204a11e3495",
   "metadata": {},
   "source": [
    "We are going to converse with a resume PDF to demonstrate this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057ed90-7e39-41e0-b2ef-fa92dba71b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 1 - Read the PDF File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820fd2c-62e2-4f9a-a45b-151222a7cc2b",
   "metadata": {},
   "source": [
    "We follow different approaches based on whether the PDF is scanned or digital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cea90-d384-4a2b-a7fa-8f00ccc21b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Approach 1 : Read Digital PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fe3148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling PyPDF2...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠹\u001b[0m ✔ Installation Succeeded\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (cdb9eb)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling pdf2image...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠹\u001b[0m ✔ Installation Succeeded.\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (cdb9eb)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling PIL...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠸\u001b[0m Installing PIL.....\u001b\u001b[1m[\u001b[0m31m\u001b\u001b[1m[\u001b[0m1mError: \u001b\u001b[1m[\u001b[0m0m An error occurred while installing \u001b\u001b[1m[\u001b[0m32mPIL\u001b\u001b[1m[\u001b[0m0m!\n",
      "Error text: \n",
      "\u001b\u001b[1m[\u001b[0m36mERROR: Could not find a version that satisfies the requirement pil \u001b[1m(\u001b[0mfrom \n",
      "versions: none\u001b[1m)\u001b[0m\n",
      "ERROR: No matching distribution found for pil\n",
      "\u001b\u001b[1m[\u001b[0m0m\n",
      "✘ Installation Failed\n",
      "\u001b[2K\u001b[32m⠼\u001b[0m Installing PIL...\n",
      "\u001b[1A\u001b[2K\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling pytesseract...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠦\u001b[0m ✔ Installation Succeededpytesseract\u001b[0m \u001b[1mto Pipfile's\u001b[0m \u001b[33m\u001b[1m\u001b[0m\u001b[1m...\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (cdb9eb)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling getpass...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠼\u001b[0m Installing getpass.....\u001b\u001b[1m[\u001b[0m31m\u001b\u001b[1m[\u001b[0m1mError: \u001b\u001b[1m[\u001b[0m0m An error occurred while installing \u001b\u001b[1m[\u001b[0m32mgetpass\u001b\u001b[1m[\u001b[0m0m!\n",
      "Error text: \n",
      "\u001b\u001b[1m[\u001b[0m36mERROR: Could not find a version that satisfies the requirement getpass \u001b[1m(\u001b[0mfrom\n",
      "versions: none\u001b[1m)\u001b[0m\n",
      "ERROR: No matching distribution found for getpass\n",
      "\u001b\u001b[1m[\u001b[0m0m\n",
      "✘ Installation Failed\n",
      "\u001b[2K\u001b[32m⠴\u001b[0m Installing getpass...\n",
      "\u001b[1A\u001b[2K\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling openai...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠇\u001b[0m ✔ Installation Succeeded\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (cdb9eb)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "!pipenv install PyPDF2\n",
    "!pipenv install pdf2image \n",
    "!pipenv install PIL \n",
    "!pipenv install pytesseract\n",
    "!pipenv install getpass\n",
    "!pipenv install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd36aa92-9c07-4c79-b107-0ac6b90b6e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTIONAL  (EXPERIENCED)   \n",
      "IM A . SAMPLE I  \n",
      "1234 North 55 Street  \n",
      "Bellevue, Nebraska 68005  \n",
      "(402) 292 -2345  \n",
      "imasample1@xxx.com  \n",
      " \n",
      "SUMMARY OF QUALIFICATIONS  \n",
      "Exceptionally well organized and resourceful Professional  with more than six years experience and a \n",
      "solid academic background in accounting and financial management; excellent analytical and problem \n",
      "solving skills; able to handle multiple projects while pr oducing high quality work in a fast -paced, \n",
      "deadline -oriented environment.  \n",
      " \n",
      "EDUCATION  \n",
      "Bachelor of Science , Bellevue University, Bellevue, NE (In Progress)  \n",
      " Major:  Accounting  Minor:  Computer Information Systems  \n",
      " Expected Graduation Date:  January, 20xx  GPA  to date:  3.95/4.00  \n",
      " \n",
      "PROFESSIONAL ACCOMPLISHMENTS  \n",
      "Accounting and Financial Management  \n",
      " Developed and maintained accounting records for up to fifty bank accounts.  \n",
      " Formulated monthly and year -end financial statements and generated various payroll records, \n",
      "including federal and state payroll reports, annual tax reports, W -2 and 1099 forms, etc.  \n",
      " Tested accuracy of account balances and prepared supporting documentation for submission during a \n",
      "comprehensive three -year audit of financial operations.  \n",
      " Formulated int ricate pro -forma budgets.  \n",
      " Calculated and implemented depreciation/amortization schedules.  \n",
      "Information Systems Analysis and Problem Solving  \n",
      " Converted manual to computerized accounting systems for two organizations.  \n",
      " Analyzed and successfully reprogrammed sof tware to meet customer requirements.  \n",
      " Researched and corrected problems to assure effective operation of newly computerized systems.  \n",
      " \n",
      "WORK HISTORY  \n",
      "Student Intern , Financial Accounting Development Program, Mutual of Omaha, Omaha, NE  \n",
      "(Summer 20xx)  \n",
      "Accounting Coordinator , Nebraska Special Olympics, Omaha, NE (20xx -20xx)  \n",
      "Bookkeeper , SMC, Inc., Omaha, NE (20xx – 20xx)  \n",
      "Bookkeeper , First United Methodist Church, Altus, OK (20xx – 20xx)  \n",
      " \n",
      "PROFESSIONAL AFFILIATION  \n",
      "Member,  IMA, Bellevue University Student Chapter  \n",
      " \n",
      "COMP UTER SKILLS  \n",
      " Proficient in MS Office (Word, Excel, PowerPoint, Outlook), QuickBooks  \n",
      " Basic Knowledge of MS Access, SQL, Visual Basic, C++  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file_obj = open('resume-sample.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "num_pages = len(pdf_reader.pages)\n",
    "detected_text = ''\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_obj = pdf_reader.pages[page_num]\n",
    "    detected_text += page_obj.extract_text() + '\\n\\n'\n",
    "\n",
    "pdf_file_obj.close()\n",
    "\n",
    "print(detected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d902485-c9b3-4872-a093-6d06aad18cf2",
   "metadata": {},
   "source": [
    "##### Approach 2 : Read Scanned PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7277de2-ca00-4ab5-8c9c-733d7ef0a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "image = pdf2image.convert_from_path('resume-sample.pdf')\n",
    "for pagenumber, page in enumerate(image):\n",
    "    detected_text = pytesseract.image_to_string(page)\n",
    "    print(detected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058399a7-019a-4cbd-a276-a45b6d7a861d",
   "metadata": {},
   "source": [
    "#### Step 2 - First Chat with PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d602b7b-1335-49fe-a13a-2243e4af3a89",
   "metadata": {},
   "source": [
    "Let us ask the LLM to suggest jobs that this person will be suitable for based on his resume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a3256-3bbf-4653-927e-88e33b8fb56b",
   "metadata": {},
   "source": [
    "Firstly, We import the os and openai library and define our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90234c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling os...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m Installing os...\u001b\u001b[1m[\u001b[0m31m\u001b\u001b[1m[\u001b[0m1mError: \u001b\u001b[1m[\u001b[0m0m An error occurred while installing \u001b\u001b[1m[\u001b[0m32mos\u001b\u001b[1m[\u001b[0m0m!\n",
      "Error text: \n",
      "\u001b\u001b[1m[\u001b[0m36mERROR: Could not find a version that satisfies the requirement os \u001b[1m(\u001b[0mfrom \n",
      "versions: none\u001b[1m)\u001b[0m\n",
      "ERROR: No matching distribution found for os\n",
      "\u001b\u001b[1m[\u001b[0m0m\n",
      "✘ Installation Failed\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m Installing os...\n",
      "\u001b[1A\u001b[2K\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling openai...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠼\u001b[0m ✔ Installation Succeeded\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (f9464d)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling getpass...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing getpass...\u001b\u001b[1m[\u001b[0m31m\u001b\u001b[1m[\u001b[0m1mError: \u001b\u001b[1m[\u001b[0m0m An error occurred while installing \u001b\u001b[1m[\u001b[0m32mgetpass\u001b\u001b[1m[\u001b[0m0m!\n",
      "Error text: \n",
      "\u001b\u001b[1m[\u001b[0m36mERROR: Could not find a version that satisfies the requirement getpass \u001b[1m(\u001b[0mfrom\n",
      "versions: none\u001b[1m)\u001b[0m\n",
      "ERROR: No matching distribution found for getpass\n",
      "\u001b\u001b[1m[\u001b[0m0m\n",
      "✘ Installation Failed\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing getpass...\n",
      "\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!pipenv install os\n",
    "!pipenv install openai\n",
    "!pipenv install getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e74c9f1-0fbb-4ffa-a296-44da367e6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass \n",
    "\n",
    "openai.api_key = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04936e0-574b-42c6-a0b9-d8a8f83368d9",
   "metadata": {},
   "source": [
    "Choosing the ideal model while using OpenAI's python library depends on your use case and specific requirements. We recommend going through the list of available models and learning the pros and cons of each of the available models. You can access the list of available models as follows - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9dc66-c557-4516-aa71-75bb3697c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "models = openai.Model.list()\n",
    "modelsdf = pd.DataFrame(models[\"data\"])\n",
    "modelsdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ab193-5ecd-48d4-a4b6-d4353262a80c",
   "metadata": {},
   "source": [
    "Next, we append our query - \"give a list of jobs suitable for the above resume\" to the extracted PDF text and send this as the user_msg. The detected_text variable already contains the data extracted from the PDF. We will simply append our query here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d225142-7b7d-4ca2-9acb-b7fd32eb22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'give a list of jobs suitable for the above resume.'\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3b7e7-20a3-4930-bb66-fa6c9fdd6dc1",
   "metadata": {},
   "source": [
    "We also add a relevant system_msg to refine the behavior of the AI assistant. In our case, a useful system message can be \"You are a helpful career advisor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5ca7a-9d31-4eb4-88ef-ac836fa3a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = 'You are a helpful career advisor.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc733f-b44c-46f3-a975-c6462b116c3a",
   "metadata": {},
   "source": [
    "We send the request to get our first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63bb745-1e83-4e0b-a59e-03b9e08e004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": user_msg}]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b908b03-b3f8-4493-8c7b-5ed9b4b12048",
   "metadata": {},
   "source": [
    "Once the request is complete, the response object will contain the response from the LLM. We can view it by accessing the 'choices' attribute in the response object as follows -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65372e2-17e4-4f25-ad14-c7225f89e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c13aec-3949-4cc3-a1cc-423247d2b1df",
   "metadata": {},
   "source": [
    "#### Step 3 : Continuing the Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9a9a3-9fea-4d39-b71c-5697ef868863",
   "metadata": {},
   "source": [
    "Often, we would want to have conversations with the LLM which are more than just a pair of a single prompt and a single response. Let us now learn how to use our past conversation history to continue the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4239ad3-27b1-4a47-a092-005132d5e991",
   "metadata": {},
   "source": [
    "To simplify the implementation, we define the following function for calling the OpenAI GPT API from now on -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6681b7-d81d-4f23-9508-0852ae81ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_chat(system_message, user_assistant_messages):\n",
    "  \n",
    "  system_msg = [{\"role\": \"system\", \"content\": system_message}]\n",
    "  \n",
    "  user_assistant_msgs = [{\"role\": \"assistant\", \"content\": user_assistant_messages[i]} if i % 2 else {\"role\": \"user\", \"content\": user_assistant_messages[i]} for i in range(len(user_assistant_messages))]\n",
    "\n",
    "  allmsgs = system_msg + user_assistant_msgs\n",
    "  response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                          messages=allmsgs)\n",
    "  \n",
    "  return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb502de-47c6-48d4-9768-f85d202a8619",
   "metadata": {},
   "source": [
    "The function accepts -\n",
    "\n",
    "* system_message (string) : This acts as the system_msg\n",
    "* user_assistant_messages (list) : This list contains user prompts and model responses in alternating order. This is also the order in which they occur in the conversation.\n",
    "\n",
    "The function internally makes the API call to generate and return a new response based on the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c10e06-53f9-4f3a-a903-abc1252e07f8",
   "metadata": {},
   "source": [
    "Let us now use this function to continue our previous conversation, and find out the highest paying jobs out of the ones recommended in the first response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f299e-6cc6-4a75-bb9c-44bd309fb2e8",
   "metadata": {},
   "source": [
    "We will use the same system message (system_msg) used in previous call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef1646-2c25-47cd-9952-001d0034f98d",
   "metadata": {},
   "source": [
    "We create user_assistant_messages list as follows - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da14a5e-ef43-4e5e-9ff5-9aeee2657620",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg1 = user_msg\n",
    "model_response1 = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "user_msg2 = 'based on the suggestions, choose the 3 jobs with highest average salary'\n",
    "user_assistant_msgs = [user_msg1, model_response1, user_msg2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5818d1-85f5-4d59-a6e6-84fa5f92a964",
   "metadata": {},
   "source": [
    "Note that we used the original prompt as the first user message (user_msg1), the response to that prompt as the first model response message (model_response1), and our new prompt as the second user message (user_msg2).\n",
    "\n",
    "Finally, we add them to the user_assistant_messages list in order of their occurrence in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ff2a6-b81c-4213-a9a2-2bb68d6ec16f",
   "metadata": {},
   "source": [
    "We now call the continue_chat() function to get the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4946e85-abe9-43e3-8ce6-75be57a91cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = continue_chat(system_msg, user_assistant_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18388927-b53e-4b69-8f53-b8d120da1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0aa66a-6707-4c33-9995-58b63d06340c",
   "metadata": {},
   "source": [
    "## Chat with Large PDFs using ChatGPT API and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9b7ce-05a2-458c-b9b6-5b98b07d4173",
   "metadata": {},
   "source": [
    "The code tutorial shown above fails for very large PDFs. Let us illustrate this with an example. We will try to chat with BCG's \"2022 Annual Sustainability Report\", a large PDF published by the Boston Consulting Group (BCG) on their general impact in the industry. We execute the code shown below -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1dfb47-ef56-4a75-a2a4-1423a00d1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file_obj = open('bcg-2022-annual-sustainability-report-apr-2023.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf_file_obj)\n",
    "num_pages = pdf_reader.numPages\n",
    "detected_text = ''\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_obj = pdf_reader.getPage(page_num)\n",
    "    detected_text += page_obj.extractText() + '\\n\\n'\n",
    "\n",
    "pdf_file_obj.close()\n",
    "print(len(detected_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d084d-04f5-4880-85f9-cb3ae664a7c0",
   "metadata": {},
   "source": [
    "We can see that the PDF is super large, and the length of the detected_text string variable is roughly 250k.\n",
    "\n",
    "Let us now try chatting with the PDF -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c4d72-ab03-4b06-afdb-5a3b37aa5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = ''\n",
    "\n",
    "query = '''\n",
    "summarize this PDF in 500 words.\n",
    "'''\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                         {\"role\": \"user\", \"content\": user_msg}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b64e1-ad09-4aa3-a633-9e2a9ae9bd18",
   "metadata": {},
   "source": [
    "We get an error message saying that we have hit the prompt length threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364eebdc-44b9-4d49-a0fb-ec0216620db6",
   "metadata": {},
   "source": [
    "This happens because for large PDFs with lots of text, the request payload we send to OpenAI becomes too large, and OpenAI returns an error saying that we have hit the prompt length threshold.\n",
    "\n",
    "Let us now learn how to remove this bottleneck.\n",
    "\n",
    "Enter LangChain. LangChain is an innovative technology that functions as a bridge -  linking large language models (LLMs) with practical applications like Python programming, PDFs, CSV files, or databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133610e2-7f07-4c12-9f97-f4ddabd6954e",
   "metadata": {},
   "source": [
    "Let us import the required dependencies and get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49eb2b-3708-4926-b0bd-4b38d901a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464f6f9-1c02-448f-a8e9-374e896cb8e7",
   "metadata": {},
   "source": [
    "We load the PDF using PyPDF loader for LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133dabb-cffa-4488-bfa2-2eaefe6eb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"bcg-2022-annual-sustainability-report-apr-2023.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c1956-221f-47ab-9628-cf1911da53fd",
   "metadata": {},
   "source": [
    "We will perform chunking and split the text using LangChain text splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94780d-8814-4991-a245-cee2019e2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f5ad8-22a1-4892-8061-6a49879ae312",
   "metadata": {},
   "source": [
    "We create a vector database using the chunks. We will save it the database for future use as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f777e-a3c8-4f92-ba74-87664c62748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798c771-1a80-4dc7-8554-dec20b73c628",
   "metadata": {
    "tags": []
   },
   "source": [
    "We now load the database. Using the database, we configure a retriever and then create a chat object. This chat object (qa_interface) will be used to chat with the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8625c8b-8cb4-4893-8c2b-0a7686ad359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba03911-8774-428a-818d-471290ae5379",
   "metadata": {},
   "source": [
    "We can now start chatting with the PDF. Let us ask the PDF to list measures taken to address diseases occurring in developing industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec8058-5ca5-463b-bca0-d82655fe927e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = qa_interface(\"List measures taken to address diseases occuring in developing industries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba31977-c58a-4ce9-9e02-ca10441abfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98144ad4-c83b-457d-a218-d83fcf2d87fd",
   "metadata": {},
   "source": [
    "So far, we've used the RetrievalQA chain, a LangChain type for pulling document pieces from a vector store and asking one question about them. But, sometimes we need to have a full conversation about a document, including referring to topics we've already talked about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d89fa5-8647-4c60-9900-ce80e63e5edf",
   "metadata": {},
   "source": [
    "Thankfully, LangChain has us covered. To make this possible, our system needs a memory or conversation history.  Instead of the RetrievalQA chain, we'll use the ConversationalRetrievalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f16b7-d173-4e71-9754-c7afab10afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_interface = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0), retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949d78f-9e99-457f-a78b-8b4ad86d926f",
   "metadata": {},
   "source": [
    "Let's ask the PDF to reveal the context in which Morocco is mentioned in the report.\n",
    "\n",
    "'chat_history' parameter is a list contains past conversation history. For the first message, this list will be empty.\n",
    "\n",
    "'question' parameter is used to send our message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea0fa8-ef31-4e8d-be1f-7606d17db122",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"in what context is Morocco mentioned in the report?\"\n",
    "result = conv_interface({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e09a97-ceee-4320-a5f3-15dbce7bb1ed",
   "metadata": {},
   "source": [
    "Let us now continue the conversation by updating the chat_history variable and ask the PDF to give some statistics around this. We append the messages in order of appearance in the conversation. We first append our initial message followed by the first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef65d2-2a17-4b22-811e-d1e3cc386654",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854acef-2c8c-4518-91d8-97b16d11d0fa",
   "metadata": {},
   "source": [
    "We now add our new question along with the updated chat_history to continue the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f84982-b5f7-4f38-8ff2-9e73663faa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give some statistics around this.\"\n",
    "result = conv_interface({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9955e9-2404-456c-a77d-ebd7073eefe4",
   "metadata": {},
   "source": [
    "The result uses the context gained by knowing the conversation history, and provides another great response! We can keep updating the chat_history variable and further continue our conversation using this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ddf19-3634-4554-b9a0-22fcaa15eef8",
   "metadata": {},
   "source": [
    "## Build PDF Automations using OpenAI GPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422d523-b6ca-4cab-b815-9003eb8fe392",
   "metadata": {},
   "source": [
    "Let us now explore automations involving PDF tasks that can be implemented using GPT API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688353d-83fb-4009-b07c-69d51224d17f",
   "metadata": {},
   "source": [
    "#### Automation 1 - Document Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ee803-ee65-4847-bd0a-b1238b4089ae",
   "metadata": {},
   "source": [
    "GPT-3.5 is excellent at extracting data from documents. Let us try to extract data from an invoice using it. We are going to extract the following fields in JSON format - invoice_date, invoice_number, seller_name, seller_address, total_amount, and each line item present in the invoice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7d3f4-57fc-44b7-b5cc-91ad48ff12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "image = pdf2image.convert_from_path('invoice.pdf')\n",
    "for pagenumber, page in enumerate(image):\n",
    "    detected_text = pytesseract.image_to_string(page)\n",
    "    \n",
    "system_msg = 'You are an invoice processing solution.'\n",
    "\n",
    "query = '''\n",
    "extract data from above invoice and return only the json containing the following -\n",
    "invoice_date, invoice_number, seller_name, seller_address, total_amount, and each line item present in the invoice.\n",
    "json=\n",
    "'''\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                         {\"role\": \"user\", \"content\": user_msg}])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c475d48-81cd-4608-9800-0e61dffec99f",
   "metadata": {},
   "source": [
    "The json here is essentially a json dump - it is a text string which is in the correct json format, but is not a json variable yet.\n",
    "\n",
    "Let us convert this response to a json variable, which happens by adding just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aef0fe-705d-48a1-a375-d257f2b5b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "invoice_json = json.loads(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03085da9-53b1-43bb-93ae-f1a9c5d394be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_json = json.dumps(invoice_json, indent=2)\n",
    "print(pretty_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1c40f-b279-4019-96e9-fc82708a7b6a",
   "metadata": {},
   "source": [
    "#### Automation 2 - Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f6647-52c5-4dde-8fb8-67f458bdd861",
   "metadata": {},
   "source": [
    "Let us consider an example. Say we have a lot of files which are either invoices or receipts. We want to classify and sort these documents based on their type.\n",
    "\n",
    "Doing this is easy using GPT API.\n",
    "\n",
    "We create simple python functions to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102e366-6a1c-4266-9da0-6d9ec447f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "\n",
    "def list_files_only(directory_path):\n",
    "    if os.path.isdir(directory_path):\n",
    "        file_list = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "        file_list = [file for file in file_list if \".pdf\" in file]\n",
    "        return file_list\n",
    "    else:\n",
    "        return f\"{directory_path} is not a directory\"\n",
    "\n",
    "def classify(file_name):\n",
    "    \n",
    "    image = pdf2image.convert_from_path(file_name)\n",
    "    for pagenumber, page in enumerate(image):\n",
    "        detected_text = pytesseract.image_to_string(page)\n",
    "    \n",
    "    system_msg = 'You are an accounts payable expert.'\n",
    "\n",
    "    query = '''\n",
    "    Classify this document and return one of these two document types as response - [Invoices, Receipts]\n",
    "    Return only the document type in the response.\n",
    "\n",
    "    Document Type = \n",
    "    '''\n",
    "\n",
    "    user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                             {\"role\": \"user\", \"content\": user_msg}])\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def move_file(current_path, new_folder):\n",
    "    if os.path.isfile(current_path) and os.path.isdir(new_folder):\n",
    "        file_name = os.path.basename(current_path)\n",
    "        new_path = os.path.join(new_folder, file_name)\n",
    "        shutil.move(current_path, new_path)\n",
    "        print(f'File moved to {new_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8e4a0-2b9e-475a-8e1a-51b97b5b77e7",
   "metadata": {},
   "source": [
    "We create two folders labelled 'Invoices' & 'Receipts', in the folder where the unclassified invoices & receipts are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27097f34-7bbe-46d1-9dd7-65db903725e2",
   "metadata": {},
   "source": [
    "Let us execute the code now to classify these files and sort them into separate folders based on the document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03846c-25b1-46e8-a166-2e6e58412f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = list_files_only('invoices and receipts/')\n",
    "for doc in list_of_files:\n",
    "    current_path = 'invoices and receipts/' + doc\n",
    "    doc_type = classify(current_path)\n",
    "    new_path = 'invoices and receipts/' + doc_type\n",
    "    move_file(current_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a55430-b88c-4d3a-bbc9-6e67f0f6051b",
   "metadata": {},
   "source": [
    "Upon execution, the code sorts these files perfectly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b610220-b182-4edb-8cf4-ed2c640daeb6",
   "metadata": {},
   "source": [
    "#### Automation 3 - Recipe Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ecc623-9789-4a51-9454-d3a1beb98395",
   "metadata": {},
   "source": [
    "We can even feed our favorite cookbooks to GPT API, and ask it to give recipe recommendations based on our inputs. Let us look at an example. We use the Brakes' Meals n More recipe cookbook, and talk to it using LangChain. Let us ask it to give recommendations based on the ingredients we have at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d44353-9b08-4d09-82c0-dde36470bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "directory = 'index_store'\n",
    "\n",
    "loader = PyPDFLoader(\"meals-more-recipes.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])\n",
    "\n",
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)\n",
    "\n",
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "response = qa_interface(\"\"\"\n",
    "I have a lot of broccoli and tomatoes at home. \n",
    "Recommend recipe for some meal I can make at home using these.\n",
    "\"\"\")\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e188d-e9f1-40b5-950f-f9aeb4d7121f",
   "metadata": {},
   "source": [
    "Upon execution, the PDF recommends a recipe for a meal that can be prepared using the mentioned ingredients!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f154b91-9bde-463b-a584-ebe146902efc",
   "metadata": {},
   "source": [
    "#### Automation 4 - Automated Test Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85030b2-0966-486d-854a-95d2bdc9d771",
   "metadata": {},
   "source": [
    "You can feed textbooks and automate creation of complete question papers and tests using GPT API. The LLM can even generate the marking scheme for you!\n",
    "We use the textbook Advanced High-School Mathematics by David B. Surowski and ask the LLM to create a question paper with a marking scheme for a particular chapter in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35da741-fc85-40b1-ae29-8d70a6e47738",
   "metadata": {},
   "source": [
    "We execute the below code - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d9202-2058-4fb5-a859-5bf213673789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "directory = 'index_store'\n",
    "\n",
    "loader = PyPDFLoader(\"further.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])\n",
    "\n",
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)\n",
    "\n",
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "response = qa_interface(\"\"\"\n",
    "list 5 questions of 20 marks total of varying difficuly and weightage based on the topic \"Euclidian Geometry\"\n",
    "\"\"\")\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f36101-9ae6-4854-8912-341873902998",
   "metadata": {},
   "source": [
    "The LLM reads the PDF textbook and create the question paper for us!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
