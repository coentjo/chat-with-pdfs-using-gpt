{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ba6108-588a-470b-8ea3-86521985ec4c",
   "metadata": {},
   "source": [
    "# Chat with PDFs using ChatGPT & OpenAI GPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1d5ed-de6d-4edb-8c02-5abe8631c5bd",
   "metadata": {},
   "source": [
    "This is a supplementary python notebook for the blog - https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/. We dive into a detailed code tutorial on how to chat with all kinds of PDF files using OpenAI GPT API and use it for PDF automations / chatbots.\n",
    "\n",
    "* We will chat with PDFs using just a few lines of Python code.\n",
    "* We will chat with large PDF files using ChatGPT API and LangChain.\n",
    "* We will build an automation to sort PDF files based on their contents.\n",
    "* We will go through examples of building more automations for tasks involving PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b5d6-dc01-4ea3-a4aa-0b9065894da3",
   "metadata": {},
   "source": [
    "## Chat with PDF using ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49317bea-fcb7-4473-bf1c-1603ca18d67b",
   "metadata": {},
   "source": [
    "Let us now chat with our first PDF using OpenAI's GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ef09a-3a36-4e14-8477-6204a11e3495",
   "metadata": {},
   "source": [
    "We are going to converse with a resume PDF to demonstrate this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057ed90-7e39-41e0-b2ef-fa92dba71b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 1 - Read the PDF File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820fd2c-62e2-4f9a-a45b-151222a7cc2b",
   "metadata": {},
   "source": [
    "We follow different approaches based on whether the PDF is scanned or digital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cea90-d384-4a2b-a7fa-8f00ccc21b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Approach 1 : Read Digital PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fe3148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling PyPDF2...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠦\u001b[0m ✔ Installation Succeeded\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (beb423)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling pdf2image...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m ✔ Installation Succeeded.\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (beb423)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling Pillow...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m ✔ Installation SucceededPillow\u001b[0m \u001b[1mto Pipfile's\u001b[0m \u001b[33m\u001b[1m\u001b[0m\u001b[1m...\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[33m\u001b[1mPipfile.lock (beb423) out of date, updating to (a275f9)...\u001b[0m\n",
      "Locking\u001b[0m \u001b[33m[packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠴\u001b[0m ✔ Success!dependencies....\n",
      "\u001b[1A\u001b[2KLocking\u001b[0m \u001b[33m[dev-packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠇\u001b[0m ✔ Success!dependencies.....\n",
      "\u001b[1A\u001b[2K\u001b[1mUpdated Pipfile.lock (08d1e96c6bbb9b3e951eebbd6cd83f53794a3836040f57090c3cd985a4a275f9)!\u001b[0m\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock (a275f9)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling pytesseract...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m ✔ Installation Succeeded...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (a275f9)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling openai...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠧\u001b[0m ✔ Installation Succeeded\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (a275f9)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "!pipenv install PyPDF2\n",
    "!pipenv install pdf2image \n",
    "!pipenv install Pillow \n",
    "!pipenv install pytesseract\n",
    "!pipenv install openai\n",
    "!pipenv install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd36aa92-9c07-4c79-b107-0ac6b90b6e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTIONAL  (EXPERIENCED)   \n",
      "IM A . SAMPLE I  \n",
      "1234 North 55 Street  \n",
      "Bellevue, Nebraska 68005  \n",
      "(402) 292 -2345  \n",
      "imasample1@xxx.com  \n",
      " \n",
      "SUMMARY OF QUALIFICATIONS  \n",
      "Exceptionally well organized and resourceful Professional  with more than six years experience and a \n",
      "solid academic background in accounting and financial management; excellent analytical and problem \n",
      "solving skills; able to handle multiple projects while pr oducing high quality work in a fast -paced, \n",
      "deadline -oriented environment.  \n",
      " \n",
      "EDUCATION  \n",
      "Bachelor of Science , Bellevue University, Bellevue, NE (In Progress)  \n",
      " Major:  Accounting  Minor:  Computer Information Systems  \n",
      " Expected Graduation Date:  January, 20xx  GPA  to date:  3.95/4.00  \n",
      " \n",
      "PROFESSIONAL ACCOMPLISHMENTS  \n",
      "Accounting and Financial Management  \n",
      " Developed and maintained accounting records for up to fifty bank accounts.  \n",
      " Formulated monthly and year -end financial statements and generated various payroll records, \n",
      "including federal and state payroll reports, annual tax reports, W -2 and 1099 forms, etc.  \n",
      " Tested accuracy of account balances and prepared supporting documentation for submission during a \n",
      "comprehensive three -year audit of financial operations.  \n",
      " Formulated int ricate pro -forma budgets.  \n",
      " Calculated and implemented depreciation/amortization schedules.  \n",
      "Information Systems Analysis and Problem Solving  \n",
      " Converted manual to computerized accounting systems for two organizations.  \n",
      " Analyzed and successfully reprogrammed sof tware to meet customer requirements.  \n",
      " Researched and corrected problems to assure effective operation of newly computerized systems.  \n",
      " \n",
      "WORK HISTORY  \n",
      "Student Intern , Financial Accounting Development Program, Mutual of Omaha, Omaha, NE  \n",
      "(Summer 20xx)  \n",
      "Accounting Coordinator , Nebraska Special Olympics, Omaha, NE (20xx -20xx)  \n",
      "Bookkeeper , SMC, Inc., Omaha, NE (20xx – 20xx)  \n",
      "Bookkeeper , First United Methodist Church, Altus, OK (20xx – 20xx)  \n",
      " \n",
      "PROFESSIONAL AFFILIATION  \n",
      "Member,  IMA, Bellevue University Student Chapter  \n",
      " \n",
      "COMP UTER SKILLS  \n",
      " Proficient in MS Office (Word, Excel, PowerPoint, Outlook), QuickBooks  \n",
      " Basic Knowledge of MS Access, SQL, Visual Basic, C++  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file_obj = open('resume-sample.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "num_pages = len(pdf_reader.pages)\n",
    "detected_text = ''\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_obj = pdf_reader.pages[page_num]\n",
    "    detected_text += page_obj.extract_text() + '\\n\\n'\n",
    "\n",
    "pdf_file_obj.close()\n",
    "\n",
    "print(detected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d902485-c9b3-4872-a093-6d06aad18cf2",
   "metadata": {},
   "source": [
    "##### Approach 2 : Read Scanned PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7277de2-ca00-4ab5-8c9c-733d7ef0a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "image = pdf2image.convert_from_path('resume-sample.pdf')\n",
    "for pagenumber, page in enumerate(image):\n",
    "    detected_text = pytesseract.image_to_string(page)\n",
    "    print(detected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058399a7-019a-4cbd-a276-a45b6d7a861d",
   "metadata": {},
   "source": [
    "#### Step 2 - First Chat with PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d602b7b-1335-49fe-a13a-2243e4af3a89",
   "metadata": {},
   "source": [
    "Let us ask the LLM to suggest jobs that this person will be suitable for based on his resume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a3256-3bbf-4653-927e-88e33b8fb56b",
   "metadata": {},
   "source": [
    "Firstly, We import the os and openai library and define our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e74c9f1-0fbb-4ffa-a296-44da367e6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass \n",
    "\n",
    "openai.api_key = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04936e0-574b-42c6-a0b9-d8a8f83368d9",
   "metadata": {},
   "source": [
    "Choosing the ideal model while using OpenAI's python library depends on your use case and specific requirements. We recommend going through the list of available models and learning the pros and cons of each of the available models. You can access the list of available models as follows - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb9dc66-c557-4516-aa71-75bb3697c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>permission</th>\n",
       "      <th>root</th>\n",
       "      <th>parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-similarity-curie-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1651172507</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>[{'id': 'modelperm-GcRzwghQAA4RMbu5mzeVLaaS', ...</td>\n",
       "      <td>text-similarity-curie-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4</td>\n",
       "      <td>model</td>\n",
       "      <td>1687882411</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-kMXwdWZD1zTr5s2zZhMZMCw0', ...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-0314</td>\n",
       "      <td>model</td>\n",
       "      <td>1687882410</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-QANFnZSlXuyuzFGziCgkrVRf', ...</td>\n",
       "      <td>gpt-4-0314</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>babbage</td>\n",
       "      <td>model</td>\n",
       "      <td>1649358449</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-h574xGeqWyBeFDDKaoVTC4CO', ...</td>\n",
       "      <td>babbage</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text-babbage-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1649364043</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-YABzYWjC1kS6M2BnI6Fr9vuS', ...</td>\n",
       "      <td>text-babbage-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>curie-similarity</td>\n",
       "      <td>model</td>\n",
       "      <td>1651172510</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>[{'id': 'modelperm-7JoCxEzsKwSxMVoWuxLpmJxR', ...</td>\n",
       "      <td>curie-similarity</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>davinci</td>\n",
       "      <td>model</td>\n",
       "      <td>1649359874</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-RcfCH2MkO5NP9C7wx6kdYnIT', ...</td>\n",
       "      <td>davinci</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>code-davinci-edit-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1649880484</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-T8Ie7SvlPyvtsDvPlfC8DftZ', ...</td>\n",
       "      <td>code-davinci-edit-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>curie-instruct-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1649364042</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-rTxpdy2DwwUp38frYQFsj5OC', ...</td>\n",
       "      <td>curie-instruct-beta</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-3.5-turbo-instruct-0914</td>\n",
       "      <td>model</td>\n",
       "      <td>1694122472</td>\n",
       "      <td>system</td>\n",
       "      <td>[{'id': 'modelperm-MrSDYnG6Tr5wLfenxPRaqccH', ...</td>\n",
       "      <td>gpt-3.5-turbo-instruct-0914</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id object     created    owned_by  \\\n",
       "0    text-similarity-curie-001  model  1651172507  openai-dev   \n",
       "1                        gpt-4  model  1687882411      openai   \n",
       "2                   gpt-4-0314  model  1687882410      openai   \n",
       "3                      babbage  model  1649358449      openai   \n",
       "4             text-babbage-001  model  1649364043      openai   \n",
       "5             curie-similarity  model  1651172510  openai-dev   \n",
       "6                      davinci  model  1649359874      openai   \n",
       "7        code-davinci-edit-001  model  1649880484      openai   \n",
       "8          curie-instruct-beta  model  1649364042      openai   \n",
       "9  gpt-3.5-turbo-instruct-0914  model  1694122472      system   \n",
       "\n",
       "                                          permission  \\\n",
       "0  [{'id': 'modelperm-GcRzwghQAA4RMbu5mzeVLaaS', ...   \n",
       "1  [{'id': 'modelperm-kMXwdWZD1zTr5s2zZhMZMCw0', ...   \n",
       "2  [{'id': 'modelperm-QANFnZSlXuyuzFGziCgkrVRf', ...   \n",
       "3  [{'id': 'modelperm-h574xGeqWyBeFDDKaoVTC4CO', ...   \n",
       "4  [{'id': 'modelperm-YABzYWjC1kS6M2BnI6Fr9vuS', ...   \n",
       "5  [{'id': 'modelperm-7JoCxEzsKwSxMVoWuxLpmJxR', ...   \n",
       "6  [{'id': 'modelperm-RcfCH2MkO5NP9C7wx6kdYnIT', ...   \n",
       "7  [{'id': 'modelperm-T8Ie7SvlPyvtsDvPlfC8DftZ', ...   \n",
       "8  [{'id': 'modelperm-rTxpdy2DwwUp38frYQFsj5OC', ...   \n",
       "9  [{'id': 'modelperm-MrSDYnG6Tr5wLfenxPRaqccH', ...   \n",
       "\n",
       "                          root parent  \n",
       "0    text-similarity-curie-001   None  \n",
       "1                        gpt-4   None  \n",
       "2                   gpt-4-0314   None  \n",
       "3                      babbage   None  \n",
       "4             text-babbage-001   None  \n",
       "5             curie-similarity   None  \n",
       "6                      davinci   None  \n",
       "7        code-davinci-edit-001   None  \n",
       "8          curie-instruct-beta   None  \n",
       "9  gpt-3.5-turbo-instruct-0914   None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "models = openai.Model.list()\n",
    "modelsdf = pd.DataFrame(models[\"data\"])\n",
    "modelsdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ab193-5ecd-48d4-a4b6-d4353262a80c",
   "metadata": {},
   "source": [
    "Next, we append our query - \"give a list of jobs suitable for the above resume\" to the extracted PDF text and send this as the user_msg. The detected_text variable already contains the data extracted from the PDF. We will simply append our query here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d225142-7b7d-4ca2-9acb-b7fd32eb22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'give a list of jobs suitable for the above resume.'\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3b7e7-20a3-4930-bb66-fa6c9fdd6dc1",
   "metadata": {},
   "source": [
    "We also add a relevant system_msg to refine the behavior of the AI assistant. In our case, a useful system message can be \"You are a helpful career advisor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d5ca7a-9d31-4eb4-88ef-ac836fa3a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = 'You are a helpful career advisor.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc733f-b44c-46f3-a975-c6462b116c3a",
   "metadata": {},
   "source": [
    "We send the request to get our first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63bb745-1e83-4e0b-a59e-03b9e08e004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": user_msg}]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b908b03-b3f8-4493-8c7b-5ed9b4b12048",
   "metadata": {},
   "source": [
    "Once the request is complete, the response object will contain the response from the LLM. We can view it by accessing the 'choices' attribute in the response object as follows -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65372e2-17e4-4f25-ad14-c7225f89e0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided resume, here is a list of suitable job positions:\n",
      "\n",
      "1. Accountant\n",
      "2. Financial Analyst\n",
      "3. Financial Manager\n",
      "4. Accounting Coordinator\n",
      "5. Bookkeeper\n",
      "6. Financial Intern\n",
      "7. Accounting Assistant\n",
      "8. Financial Reporting Specialist\n",
      "9. Budget Analyst\n",
      "10. Payroll Analyst\n",
      "\n",
      "These positions align with the candidate's experience in accounting and financial management, as well as their strong organizational and analytical skills. The candidate's knowledge of computer systems and software also opens up opportunities in the information systems analysis field.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c13aec-3949-4cc3-a1cc-423247d2b1df",
   "metadata": {},
   "source": [
    "#### Step 3 : Continuing the Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9a9a3-9fea-4d39-b71c-5697ef868863",
   "metadata": {},
   "source": [
    "Often, we would want to have conversations with the LLM which are more than just a pair of a single prompt and a single response. Let us now learn how to use our past conversation history to continue the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4239ad3-27b1-4a47-a092-005132d5e991",
   "metadata": {},
   "source": [
    "To simplify the implementation, we define the following function for calling the OpenAI GPT API from now on -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee6681b7-d81d-4f23-9508-0852ae81ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_chat(system_message, user_assistant_messages):\n",
    "  \n",
    "  system_msg = [{\"role\": \"system\", \"content\": system_message}]\n",
    "  \n",
    "  user_assistant_msgs = [{\"role\": \"assistant\", \"content\": user_assistant_messages[i]} if i % 2 else {\"role\": \"user\", \"content\": user_assistant_messages[i]} for i in range(len(user_assistant_messages))]\n",
    "\n",
    "  allmsgs = system_msg + user_assistant_msgs\n",
    "  response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                          messages=allmsgs)\n",
    "  \n",
    "  return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb502de-47c6-48d4-9768-f85d202a8619",
   "metadata": {},
   "source": [
    "The function accepts -\n",
    "\n",
    "* system_message (string) : This acts as the system_msg\n",
    "* user_assistant_messages (list) : This list contains user prompts and model responses in alternating order. This is also the order in which they occur in the conversation.\n",
    "\n",
    "The function internally makes the API call to generate and return a new response based on the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c10e06-53f9-4f3a-a903-abc1252e07f8",
   "metadata": {},
   "source": [
    "Let us now use this function to continue our previous conversation, and find out the highest paying jobs out of the ones recommended in the first response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f299e-6cc6-4a75-bb9c-44bd309fb2e8",
   "metadata": {},
   "source": [
    "We will use the same system message (system_msg) used in previous call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef1646-2c25-47cd-9952-001d0034f98d",
   "metadata": {},
   "source": [
    "We create user_assistant_messages list as follows - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7da14a5e-ef43-4e5e-9ff5-9aeee2657620",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/Chat using OpenAI API.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m user_msg1 \u001b[39m=\u001b[39m user_msg\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_response1 \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39;49m\u001b[39mchoices\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m user_msg2 \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbased on the suggestions, choose the 3 jobs with highest average salary\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m user_assistant_msgs \u001b[39m=\u001b[39m [user_msg1, model_response1, user_msg2]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "user_msg1 = user_msg\n",
    "model_response1 = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "user_msg2 = 'based on the suggestions, choose the 3 jobs with highest average salary'\n",
    "user_assistant_msgs = [user_msg1, model_response1, user_msg2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5818d1-85f5-4d59-a6e6-84fa5f92a964",
   "metadata": {},
   "source": [
    "Note that we used the original prompt as the first user message (user_msg1), the response to that prompt as the first model response message (model_response1), and our new prompt as the second user message (user_msg2).\n",
    "\n",
    "Finally, we add them to the user_assistant_messages list in order of their occurrence in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ff2a6-b81c-4213-a9a2-2bb68d6ec16f",
   "metadata": {},
   "source": [
    "We now call the continue_chat() function to get the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4946e85-abe9-43e3-8ce6-75be57a91cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = continue_chat(system_msg, user_assistant_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18388927-b53e-4b69-8f53-b8d120da1851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the suggestions, here are three job positions known to have higher average salaries:\n",
      "\n",
      "1. Financial Manager: Financial managers are responsible for overseeing the financial operations of an organization, including financial planning, budgeting, and financial reporting. The average annual salary for financial managers in the United States is around $127,990 according to the Bureau of Labor Statistics.\n",
      "\n",
      "2. Financial Analyst: Financial analysts assess the financial performance of companies and industries to provide recommendations and insights to improve profitability. The average annual salary for financial analysts is around $81,590 according to the Bureau of Labor Statistics.\n",
      "\n",
      "3. Budget Analyst: Budget analysts help organizations develop and manage their budgets by assessing financial data, monitoring spending, and making recommendations for cost reductions or resource allocation. The average annual salary for budget analysts is around $76,540 according to the Bureau of Labor Statistics.\n",
      "\n",
      "Please note that salaries can vary depending on factors such as location, industry, company size, and level of experience.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0aa66a-6707-4c33-9995-58b63d06340c",
   "metadata": {},
   "source": [
    "## Chat with Large PDFs using ChatGPT API and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9b7ce-05a2-458c-b9b6-5b98b07d4173",
   "metadata": {},
   "source": [
    "The code tutorial shown above fails for very large PDFs. Let us illustrate this with an example. We will try to chat with BCG's \"2022 Annual Sustainability Report\", a large PDF published by the Boston Consulting Group (BCG) on their general impact in the industry. We execute the code shown below -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc1dfb47-ef56-4a75-a2a4-1423a00d1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251848\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file_obj = open('bcg-2022-annual-sustainability-report-apr-2023.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "num_pages = len(pdf_reader.pages)\n",
    "detected_text = ''\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_obj = pdf_reader.pages[page_num]\n",
    "    detected_text += page_obj.extract_text() + '\\n\\n'\n",
    "\n",
    "pdf_file_obj.close()\n",
    "print(len(detected_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d084d-04f5-4880-85f9-cb3ae664a7c0",
   "metadata": {},
   "source": [
    "We can see that the PDF is super large, and the length of the detected_text string variable is roughly 250k.\n",
    "\n",
    "Let us now try chatting with the PDF -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "972c4d72-ab03-4b06-afdb-5a3b37aa5985",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 54957 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/Chat using OpenAI API.ipynb Cell 44\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39msummarize this PDF in 500 words.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m'''\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X63sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m user_msg \u001b[39m=\u001b[39m detected_text \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m query\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X63sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X63sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                         messages\u001b[39m=\u001b[39;49m[{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: system_msg},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#X63sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                          {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: user_msg}])\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 54957 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "system_msg = ''\n",
    "\n",
    "query = '''\n",
    "summarize this PDF in 500 words.\n",
    "'''\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                         {\"role\": \"user\", \"content\": user_msg}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b64e1-ad09-4aa3-a633-9e2a9ae9bd18",
   "metadata": {},
   "source": [
    "We get an error message saying that we have hit the prompt length threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364eebdc-44b9-4d49-a0fb-ec0216620db6",
   "metadata": {},
   "source": [
    "This happens because for large PDFs with lots of text, the request payload we send to OpenAI becomes too large, and OpenAI returns an error saying that we have hit the prompt length threshold.\n",
    "\n",
    "Let us now learn how to remove this bottleneck.\n",
    "\n",
    "Enter LangChain. LangChain is an innovative technology that functions as a bridge -  linking large language models (LLMs) with practical applications like Python programming, PDFs, CSV files, or databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133610e2-7f07-4c12-9f97-f4ddabd6954e",
   "metadata": {},
   "source": [
    "Let us import the required dependencies and get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "420e2502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43430.18s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling langchain...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m ✔ Installation Succeeded.\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock (985157)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "43479.99s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[32m\u001b[1mInstalling pypdf...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠼\u001b[0m ✔ Installation Succeeded\n",
      "\u001b[1A\u001b[2K\u001b[33m\u001b[1mPipfile.lock (985157) out of date, updating to (62eb95)...\u001b[0m\n",
      "Locking\u001b[0m \u001b[33m[packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m ✔ Success!dependencies....\n",
      "\u001b[1A\u001b[2KLocking\u001b[0m \u001b[33m[dev-packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠧\u001b[0m ✔ Success!dependencies.....\n",
      "\u001b[1A\u001b[2K\u001b[1mUpdated Pipfile.lock (29a7c1c9999003da08b35b8cfb66fc1cb45526108eff8667e6f87f56d762eb95)!\u001b[0m\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock (62eb95)...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "!pipenv install langchain\n",
    "!pipenv install pypdf\n",
    "!pipenv install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c49eb2b-3708-4926-b0bd-4b38d901a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464f6f9-1c02-448f-a8e9-374e896cb8e7",
   "metadata": {},
   "source": [
    "We load the PDF using PyPDF loader for LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f133dabb-cffa-4488-bfa2-2eaefe6eb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"bcg-2022-annual-sustainability-report-apr-2023.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c1956-221f-47ab-9628-cf1911da53fd",
   "metadata": {},
   "source": [
    "We will perform chunking and split the text using LangChain text splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd94780d-8814-4991-a245-cee2019e2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f5ad8-22a1-4892-8061-6a49879ae312",
   "metadata": {},
   "source": [
    "We create a vector database using the chunks. We will save it the database for future use as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "591f777e-a3c8-4f92-ba74-87664c62748e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:322\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/Chat using OpenAI API.ipynb Cell 55\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m directory \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mindex_store\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vector_index \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39;49mfrom_documents(texts, OpenAIEmbeddings())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/croc02/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat%20with%20PDFs/Chat%20using%20OpenAI%20API.ipynb#Y106sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vector_index\u001b[39m.\u001b[39msave_local(directory)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/vectorstores/base.py:417\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    416\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39;49mmetadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/vectorstores/faiss.py:602\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[1;32m    577\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    583\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[1;32m    584\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \n\u001b[1;32m    586\u001b[0m \u001b[39m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[1;32m    603\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__from(\n\u001b[1;32m    604\u001b[0m         texts,\n\u001b[1;32m    605\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    610\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \n\u001b[1;32m    473\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Dropbox/areas/ai.chat.with.PDFs/nanonets/chat-with-pdfs-using-gpt/Chat with PDFs/.venv/lib/python3.11/site-packages/langchain/embeddings/openai.py:324\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to for OpenAIEmbeddings. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    331\u001b[0m indices \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798c771-1a80-4dc7-8554-dec20b73c628",
   "metadata": {
    "tags": []
   },
   "source": [
    "We now load the database. Using the database, we configure a retriever and then create a chat object. This chat object (qa_interface) will be used to chat with the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8625c8b-8cb4-4893-8c2b-0a7686ad359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba03911-8774-428a-818d-471290ae5379",
   "metadata": {},
   "source": [
    "We can now start chatting with the PDF. Let us ask the PDF to list measures taken to address diseases occurring in developing industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec8058-5ca5-463b-bca0-d82655fe927e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = qa_interface(\"List measures taken to address diseases occuring in developing industries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba31977-c58a-4ce9-9e02-ca10441abfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98144ad4-c83b-457d-a218-d83fcf2d87fd",
   "metadata": {},
   "source": [
    "So far, we've used the RetrievalQA chain, a LangChain type for pulling document pieces from a vector store and asking one question about them. But, sometimes we need to have a full conversation about a document, including referring to topics we've already talked about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d89fa5-8647-4c60-9900-ce80e63e5edf",
   "metadata": {},
   "source": [
    "Thankfully, LangChain has us covered. To make this possible, our system needs a memory or conversation history.  Instead of the RetrievalQA chain, we'll use the ConversationalRetrievalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f16b7-d173-4e71-9754-c7afab10afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_interface = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0), retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949d78f-9e99-457f-a78b-8b4ad86d926f",
   "metadata": {},
   "source": [
    "Let's ask the PDF to reveal the context in which Morocco is mentioned in the report.\n",
    "\n",
    "'chat_history' parameter is a list contains past conversation history. For the first message, this list will be empty.\n",
    "\n",
    "'question' parameter is used to send our message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea0fa8-ef31-4e8d-be1f-7606d17db122",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"in what context is Morocco mentioned in the report?\"\n",
    "result = conv_interface({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e09a97-ceee-4320-a5f3-15dbce7bb1ed",
   "metadata": {},
   "source": [
    "Let us now continue the conversation by updating the chat_history variable and ask the PDF to give some statistics around this. We append the messages in order of appearance in the conversation. We first append our initial message followed by the first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef65d2-2a17-4b22-811e-d1e3cc386654",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854acef-2c8c-4518-91d8-97b16d11d0fa",
   "metadata": {},
   "source": [
    "We now add our new question along with the updated chat_history to continue the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f84982-b5f7-4f38-8ff2-9e73663faa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give some statistics around this.\"\n",
    "result = conv_interface({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9955e9-2404-456c-a77d-ebd7073eefe4",
   "metadata": {},
   "source": [
    "The result uses the context gained by knowing the conversation history, and provides another great response! We can keep updating the chat_history variable and further continue our conversation using this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ddf19-3634-4554-b9a0-22fcaa15eef8",
   "metadata": {},
   "source": [
    "## Build PDF Automations using OpenAI GPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422d523-b6ca-4cab-b815-9003eb8fe392",
   "metadata": {},
   "source": [
    "Let us now explore automations involving PDF tasks that can be implemented using GPT API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688353d-83fb-4009-b07c-69d51224d17f",
   "metadata": {},
   "source": [
    "#### Automation 1 - Document Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ee803-ee65-4847-bd0a-b1238b4089ae",
   "metadata": {},
   "source": [
    "GPT-3.5 is excellent at extracting data from documents. Let us try to extract data from an invoice using it. We are going to extract the following fields in JSON format - invoice_date, invoice_number, seller_name, seller_address, total_amount, and each line item present in the invoice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7d3f4-57fc-44b7-b5cc-91ad48ff12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "image = pdf2image.convert_from_path('invoice.pdf')\n",
    "for pagenumber, page in enumerate(image):\n",
    "    detected_text = pytesseract.image_to_string(page)\n",
    "    \n",
    "system_msg = 'You are an invoice processing solution.'\n",
    "\n",
    "query = '''\n",
    "extract data from above invoice and return only the json containing the following -\n",
    "invoice_date, invoice_number, seller_name, seller_address, total_amount, and each line item present in the invoice.\n",
    "json=\n",
    "'''\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                         {\"role\": \"user\", \"content\": user_msg}])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c475d48-81cd-4608-9800-0e61dffec99f",
   "metadata": {},
   "source": [
    "The json here is essentially a json dump - it is a text string which is in the correct json format, but is not a json variable yet.\n",
    "\n",
    "Let us convert this response to a json variable, which happens by adding just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aef0fe-705d-48a1-a375-d257f2b5b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "invoice_json = json.loads(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03085da9-53b1-43bb-93ae-f1a9c5d394be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_json = json.dumps(invoice_json, indent=2)\n",
    "print(pretty_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1c40f-b279-4019-96e9-fc82708a7b6a",
   "metadata": {},
   "source": [
    "#### Automation 2 - Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f6647-52c5-4dde-8fb8-67f458bdd861",
   "metadata": {},
   "source": [
    "Let us consider an example. Say we have a lot of files which are either invoices or receipts. We want to classify and sort these documents based on their type.\n",
    "\n",
    "Doing this is easy using GPT API.\n",
    "\n",
    "We create simple python functions to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102e366-6a1c-4266-9da0-6d9ec447f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "\n",
    "def list_files_only(directory_path):\n",
    "    if os.path.isdir(directory_path):\n",
    "        file_list = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "        file_list = [file for file in file_list if \".pdf\" in file]\n",
    "        return file_list\n",
    "    else:\n",
    "        return f\"{directory_path} is not a directory\"\n",
    "\n",
    "def classify(file_name):\n",
    "    \n",
    "    image = pdf2image.convert_from_path(file_name)\n",
    "    for pagenumber, page in enumerate(image):\n",
    "        detected_text = pytesseract.image_to_string(page)\n",
    "    \n",
    "    system_msg = 'You are an accounts payable expert.'\n",
    "\n",
    "    query = '''\n",
    "    Classify this document and return one of these two document types as response - [Invoices, Receipts]\n",
    "    Return only the document type in the response.\n",
    "\n",
    "    Document Type = \n",
    "    '''\n",
    "\n",
    "    user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                             {\"role\": \"user\", \"content\": user_msg}])\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def move_file(current_path, new_folder):\n",
    "    if os.path.isfile(current_path) and os.path.isdir(new_folder):\n",
    "        file_name = os.path.basename(current_path)\n",
    "        new_path = os.path.join(new_folder, file_name)\n",
    "        shutil.move(current_path, new_path)\n",
    "        print(f'File moved to {new_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8e4a0-2b9e-475a-8e1a-51b97b5b77e7",
   "metadata": {},
   "source": [
    "We create two folders labelled 'Invoices' & 'Receipts', in the folder where the unclassified invoices & receipts are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27097f34-7bbe-46d1-9dd7-65db903725e2",
   "metadata": {},
   "source": [
    "Let us execute the code now to classify these files and sort them into separate folders based on the document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03846c-25b1-46e8-a166-2e6e58412f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = list_files_only('invoices and receipts/')\n",
    "for doc in list_of_files:\n",
    "    current_path = 'invoices and receipts/' + doc\n",
    "    doc_type = classify(current_path)\n",
    "    new_path = 'invoices and receipts/' + doc_type\n",
    "    move_file(current_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a55430-b88c-4d3a-bbc9-6e67f0f6051b",
   "metadata": {},
   "source": [
    "Upon execution, the code sorts these files perfectly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b610220-b182-4edb-8cf4-ed2c640daeb6",
   "metadata": {},
   "source": [
    "#### Automation 3 - Recipe Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ecc623-9789-4a51-9454-d3a1beb98395",
   "metadata": {},
   "source": [
    "We can even feed our favorite cookbooks to GPT API, and ask it to give recipe recommendations based on our inputs. Let us look at an example. We use the Brakes' Meals n More recipe cookbook, and talk to it using LangChain. Let us ask it to give recommendations based on the ingredients we have at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d44353-9b08-4d09-82c0-dde36470bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "directory = 'index_store'\n",
    "\n",
    "loader = PyPDFLoader(\"meals-more-recipes.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])\n",
    "\n",
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)\n",
    "\n",
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "response = qa_interface(\"\"\"\n",
    "I have a lot of broccoli and tomatoes at home. \n",
    "Recommend recipe for some meal I can make at home using these.\n",
    "\"\"\")\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e188d-e9f1-40b5-950f-f9aeb4d7121f",
   "metadata": {},
   "source": [
    "Upon execution, the PDF recommends a recipe for a meal that can be prepared using the mentioned ingredients!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f154b91-9bde-463b-a584-ebe146902efc",
   "metadata": {},
   "source": [
    "#### Automation 4 - Automated Test Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85030b2-0966-486d-854a-95d2bdc9d771",
   "metadata": {},
   "source": [
    "You can feed textbooks and automate creation of complete question papers and tests using GPT API. The LLM can even generate the marking scheme for you!\n",
    "We use the textbook Advanced High-School Mathematics by David B. Surowski and ask the LLM to create a question paper with a marking scheme for a particular chapter in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35da741-fc85-40b1-ae29-8d70a6e47738",
   "metadata": {},
   "source": [
    "We execute the below code - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d9202-2058-4fb5-a859-5bf213673789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "directory = 'index_store'\n",
    "\n",
    "loader = PyPDFLoader(\"further.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])\n",
    "\n",
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)\n",
    "\n",
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "response = qa_interface(\"\"\"\n",
    "list 5 questions of 20 marks total of varying difficuly and weightage based on the topic \"Euclidian Geometry\"\n",
    "\"\"\")\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f36101-9ae6-4854-8912-341873902998",
   "metadata": {},
   "source": [
    "The LLM reads the PDF textbook and create the question paper for us!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
